# ML Patient Evaluator API

A Python Flask service providing AI-powered mental health assessments using a PyTorch multi-label model. Clean, service-scoped compose/Dockerfiles, and per-service Postman collections.

## üèóÔ∏è Project Overview

This service processes 201-item mental health questionnaire responses and predicts 19 different mental health conditions using a trained PyTorch neural network model.

## üöÄ Architecture

### Technology Stack

- **Framework**: Flask 3.1.1
- **Language**: Python 3.11+
- **ML Framework**: PyTorch 2.7.1
- **Scientific Computing**: NumPy 2.2.6
- **Testing**: pytest 8.0.0, pytest-flask 1.3.0
- **Containerization**: Docker + docker-compose per service
- **Monitoring**: Health/metrics endpoints

### Core Features

- PyTorch MultiLabelNN with 201 inputs ‚Üí 19 condition predictions
- REST API: `/predict`, `/health`, `/metrics`, `/metrics/reset`
- Error handling for missing model and invalid inputs
- Security headers; input validation handled at endpoint
- Metrics aggregation (requests, latency, success rate)

## üîß Development Setup

### Prerequisites

- Python 3.11+
- Docker and Docker Compose (optional for local container runs)
- pip

### Installation

1. Install dependencies

```bash
cd ml-patient-evaluator-api
pip install -r requirements.txt
```

2. Ensure ML model file is present

```bash
# Place the model under models/
models/mental_model_config2.pt
```

3. Run

```bash
# Local
python api.py  # http://localhost:10002

# Or via Docker
docker compose up --build
```

## üöÄ Deployment

### Production Run

```bash
# Using Gunicorn (recommended for production)
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:10002 api:app

# Or using uWSGI
pip install uwsgi
uwsgi --http :10002 --wsgi-file api.py --callable app --processes 4
```

### Environment Variables

Create `.env` file or set environment variables:

```bash
FLASK_ENV=production
PORT=10002
MODEL_PATH=models/mental_model_config2.pt
LOG_LEVEL=INFO
```

### Docker Deployment

```bash
# Build image
docker build -t ml-patient-evaluator-api .

# Run container
docker run -p 10002:10002 \
  -e FLASK_ENV=production \
  -v $(pwd)/models:/app/models \
  ml-patient-evaluator-api
```

Or using Docker Compose:

```bash
docker-compose up -d
```

### Platform-Specific Deployment

#### Railway
1. Connect GitHub repository
2. Set build command: `pip install -r requirements.txt`
3. Set start command: `gunicorn -w 4 -b 0.0.0.0:$PORT api:app`
4. Configure environment variables
5. Ensure model file is included in repository or mounted as volume

#### Render
1. Connect repository
2. Set build command: `pip install -r requirements.txt`
3. Set start command: `gunicorn -w 4 -b 0.0.0.0:$PORT api:app`
4. Configure environment variables
5. Deploy

#### AWS/GCP
- Use Docker containers with ECS/Cloud Run
- Mount model file from S3/Cloud Storage
- Configure health checks on `/health` endpoint
- Set up auto-scaling based on request volume

#### Self-Hosted (Systemd)
```bash
# Create systemd service file
sudo nano /etc/systemd/system/ml-patient-evaluator.service

# Service file content:
[Unit]
Description=ML Patient Evaluator API
After=network.target

[Service]
User=www-data
WorkingDirectory=/path/to/ml-patient-evaluator-api
Environment="PATH=/path/to/venv/bin"
ExecStart=/path/to/venv/bin/gunicorn -w 4 -b 0.0.0.0:10002 api:app
Restart=always

[Install]
WantedBy=multi-user.target

# Enable and start service
sudo systemctl enable ml-patient-evaluator
sudo systemctl start ml-patient-evaluator
```

### Health Check

The service provides health check endpoints:
- **GET** `/health` - Returns service and model status
- **GET** `/metrics` - Returns performance metrics

## üß™ Postman Collections

- This service‚Äôs collection lives here: `postman-collections/ML_Patient_Evaluator.postman_collection.json`
- Mentara API collections were moved to: `../mentara-api/postman-collections/`

## üõ†Ô∏è Development Commands

### Using Make (Recommended)

```bash
# Development
make dev                 # Start development server
make dev-debug          # Start with debug mode
make validate-model     # Validate ML model file

# Docker Compose
make compose-up         # Start services with docker-compose
make compose-up-d       # Start services in background
make compose-down       # Stop and remove containers
make compose-logs       # View container logs

# Code Quality
make lint               # Run flake8 linting
make format             # Format code with black
make type-check         # Run mypy type checking
make security-scan      # Run security audit

# Environment
make setup-env          # Setup environment variables
make setup-dev          # Complete development setup
```

### Run

```bash
python api.py           # Start development server (default port 10002)
FLASK_DEBUG=1 python api.py  # Debug mode
```

## API Endpoints

### POST /predict

Process 201-item questionnaire and get mental health predictions.

**Request:** JSON with 201 numeric values (range typically -10..10).

Examples:

1. All zeros (baseline)

```json
{
  "inputs": [
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0
  ]
}
```

2. Mild uniform

```json
{
  "inputs": [
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2
  ]
}
```

3. Stress/Anxiety elevated (GAD7 69‚Äì75; PSS 174‚Äì183)

```json
{
  "inputs": [
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,
    0.9, 0.9, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
    0.2, 0.2, 0.2, 0.2, 0.2, 0.2
  ]
}
```

**Response (Success):**

```json
{
  "Has_Depression": true,
  "Has_Anxiety": false,
  "Has_PTSD": true
  // ... 19 total conditions
}
```

**Response (Error):**

```json
{
  "error": "Model not available. Please check server logs for model loading issues."
}
```

### GET /health

Check service and model status.

**Response:**

```json
{
  "status": "healthy",
  "model_loaded": true,
  "service": "Mental Health AI Prediction Service",
  "version": "1.0.0"
}
```

### GET / (optional)

Service info if exposed.

## Questionnaire Mapping

The 201 input values correspond to specific mental health assessment tools:

| Assessment | Index Range | Items |
| ---------- | ----------- | ----- |
| PHQ        | 0-14        | 15    |
| ASRS       | 15-32       | 18    |
| AUDIT      | 33-42       | 10    |
| BES        | 43-58       | 16    |
| DAST10     | 59-68       | 10    |
| GAD7       | 69-75       | 7     |
| ISI        | 76-82       | 7     |
| MBI        | 83-104      | 22    |
| MDQ        | 105-119     | 15    |
| OCI_R      | 120-137     | 18    |
| PCL5       | 138-157     | 20    |
| PDSS       | 158-164     | 7     |
| PHQ9       | 165-173     | 9     |
| PSS        | 174-183     | 10    |
| SPIN       | 184-200     | 17    |

## Predicted Conditions

The model predicts 19 mental health conditions:

1. Has_Phobia
2. Has_Agoraphobia
3. Has_BloodPhobia
4. Has_SocialPhobia
5. Has_ADHD
6. Has_Alcohol_Problem
7. Has_Binge_Eating
8. Has_Drug_Problem
9. Has_Anxiety
10. Has_Insomnia
11. Has_Burnout
12. Has_Bipolar
13. Has_OCD
14. Has_Hoarding
15. Has_PTSD
16. Has_Panic_Disorder
17. Has_Depression
18. Has_High_Stress
19. Has_Social_Anxiety

## Notes

- Input index ranges are documented below to help craft targeted test payloads.

## Model File

The service requires `models/mental_model_config2.pt`. If missing, `/predict` returns HTTP 503.

## Error Handling

- **Model Not Loaded**: Returns 503 with descriptive error message
- **Invalid Input**: Returns 400 with validation error details
- **Wrong Input Length**: Returns 400 specifying expected length (201)
- **Non-Numeric Input**: Returns 400 with format error

## Integration with Backend

This service integrates with the NestJS backend through the `AiServiceClient` class in the pre-assessment module. The backend flattens questionnaire responses into the 201-item format and sends them to this service for prediction.

## Project Structure

```
ml-patient-evaluator-api/
‚îú‚îÄ‚îÄ api.py                      # App entry (registers blueprints)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                  # Flask factory and security headers
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ multilabel_nn.py    # PyTorch model definition
‚îÇ   ‚îî‚îÄ‚îÄ security/
‚îÇ       ‚îî‚îÄ‚îÄ security.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ mental_model_config2.pt
‚îú‚îÄ‚îÄ postman-collections/
‚îÇ   ‚îî‚îÄ‚îÄ ML_Patient_Evaluator.postman_collection.json
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### Performance Targets

- Average prediction latency: < 1000ms
- Maximum latency: < 2000ms
- Concurrent request success rate: ‚â• 80%

### Performance Targets

- Average prediction latency: < 1000ms
- Maximum latency: < 2000ms
- Memory growth: < 100MB over 100 requests
- Concurrent request success rate: ‚â• 80%

## Monitoring & Logging

Includes startup logging for model load state, health/metrics endpoints, and basic audit logging hooks in security layer.

## Security Considerations

- Input validation and sanitization
- Error messages don't expose internal details
- No sensitive data logging
- Service designed to run in containerized environment
